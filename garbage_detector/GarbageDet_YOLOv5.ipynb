{"cells":[{"cell_type":"markdown","source":["# YOLOv5 Custom Training 🚀"],"metadata":{"id":"5AcaEymVcrQ3"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"7BhDOxTHjCXp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689344645492,"user_tz":-120,"elapsed":14496,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"}},"outputId":"c65a18f2-f0a6-44fd-d5ab-142f89f01755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"yNveqeA1KXGy"},"source":["## Install Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTvDNSILZoN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689344664857,"user_tz":-120,"elapsed":16677,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"}},"outputId":"2a71ab81-0740-4f4f-c8d2-ff5a62c6e3cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15814, done.\u001b[K\n","remote: Counting objects: 100% (46/46), done.\u001b[K\n","remote: Compressing objects: 100% (44/44), done.\u001b[K\n","remote: Total 15814 (delta 9), reused 23 (delta 2), pack-reused 15768\u001b[K\n","Receiving objects: 100% (15814/15814), 14.64 MiB | 28.08 MiB/s, done.\n","Resolving deltas: 100% (10821/10821), done.\n","/content/yolov5\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.1/629.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hSetup complete. Using torch 2.0.1+cu118 (Tesla T4)\n"]}],"source":["#clone YOLOv5 and\n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow\n","\n","import torch\n","import os\n","from IPython.display import Image, clear_output  # to display images\n","\n","print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"]},{"cell_type":"markdown","metadata":{"id":"zP6USLgz2f0r"},"source":["## Roboflow Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2wGvjd4Z_92"},"outputs":[],"source":["!pip install roboflow>=1.0.1\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"ZG73C7nrLF5yXM6t29E3\",model_format=\"yolov5\", notebook=\"ultralytics\")"]},{"cell_type":"code","source":["!mkdir /content/datasets\n","!cd /content/datasets"],"metadata":{"id":"wBQNci7JtvBm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689344669773,"user_tz":-120,"elapsed":321,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"}},"outputId":"e20d2f75-11a6-4b18-bb75-d5dea81321b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/datasets’: File exists\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jjT5uIHo6l5"},"outputs":[],"source":["# set up environment\n","os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""]},{"cell_type":"code","source":["from roboflow import Roboflow\n","rf = Roboflow(api_key=\"ZG73C7nrLF5yXM6t29E3\")\n","project = rf.workspace(\"sappia\").project(\"hri-ra-mtc9l\")\n","dataset = project.version(3).download(\"yolov5\")\n","\n","\n","%cd /content/yolov5"],"metadata":{"id":"5JaIKi81eqzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X7yAi9hd-T4B"},"source":["## Train ⚡\n","\n","Here, we are able to pass a number of arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** Our dataset location is saved in the `dataset.location`\n","- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n","- **cache:** cache images for faster training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaFNnxLJbq4J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689345322685,"user_tz":-120,"elapsed":634104,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"}},"outputId":"f2ce703c-6bb9-4bbe-c998-dbc560b6676b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=/content/datasets/HRI-RA-3/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=32, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-193-g485da42 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 18.6MB/s]\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt to yolov5m.pt...\n","100% 40.8M/40.8M [00:00<00:00, 205MB/s]\n","\n","Overriding model.yaml nc=80 with nc=4\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     36369  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","Model summary: 291 layers, 20883441 parameters, 20883441 gradients, 48.3 GFLOPs\n","\n","Transferred 475/481 items from yolov5m.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/HRI-RA-3/train/labels... 1173 images, 0 backgrounds, 0 corrupt: 100% 1173/1173 [00:00<00:00, 1656.60it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/HRI-RA-3/train/labels.cache\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.3GB ram): 100% 1173/1173 [00:05<00:00, 218.61it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/HRI-RA-3/valid/labels... 50 images, 0 backgrounds, 0 corrupt: 100% 50/50 [00:00<00:00, 595.02it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/HRI-RA-3/valid/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 50/50 [00:00<00:00, 110.94it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.28 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to runs/train/exp/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp\u001b[0m\n","Starting training for 20 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/19      11.8G    0.08838    0.04584     0.0452        124        640: 100% 37/37 [00:32<00:00,  1.13it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.82s/it]\n","                   all         50         50      0.112      0.314      0.114      0.052\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/19      13.7G     0.0537    0.03559    0.03786        105        640: 100% 37/37 [00:27<00:00,  1.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.38it/s]\n","                   all         50         50      0.276      0.543      0.407      0.234\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/19      13.7G    0.04802    0.03092     0.0307        142        640: 100% 37/37 [00:27<00:00,  1.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.27it/s]\n","                   all         50         50      0.528      0.579      0.513      0.286\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/19      13.8G    0.04361    0.02882    0.02656        116        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.37s/it]\n","                   all         50         50      0.606        0.5      0.526      0.243\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/19      13.8G    0.03935    0.02863    0.02297        131        640: 100% 37/37 [00:26<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.11s/it]\n","                   all         50         50       0.53      0.233      0.316      0.142\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/19      13.8G    0.03325    0.02738    0.01966        119        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.02it/s]\n","                   all         50         50      0.586      0.716      0.714      0.464\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/19      13.8G    0.03209    0.02654    0.01693        123        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.20s/it]\n","                   all         50         50      0.529      0.568      0.524      0.318\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/19      13.8G    0.03004    0.02669    0.01703        108        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.08s/it]\n","                   all         50         50      0.479       0.59      0.589      0.342\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/19      13.8G    0.02928    0.02662    0.01675        103        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.29it/s]\n","                   all         50         50      0.737      0.553       0.66      0.396\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/19      13.8G    0.02783     0.0261    0.01555        141        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.41it/s]\n","                   all         50         50      0.831      0.818      0.841      0.606\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/19      13.8G    0.02574    0.02463     0.0137        122        640: 100% 37/37 [00:27<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.39it/s]\n","                   all         50         50      0.833      0.707       0.79      0.517\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/19      13.8G    0.02604    0.02507     0.0136        127        640: 100% 37/37 [00:27<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.39it/s]\n","                   all         50         50      0.729      0.781      0.781      0.542\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/19      13.8G     0.0254    0.02378    0.01089        112        640: 100% 37/37 [00:27<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.40it/s]\n","                   all         50         50      0.568      0.785      0.757      0.576\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/19      13.8G    0.02347    0.02323    0.01119        105        640: 100% 37/37 [00:27<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.31it/s]\n","                   all         50         50      0.887      0.825      0.888      0.661\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      14/19      13.8G    0.02303    0.02356    0.01156        109        640: 100% 37/37 [00:26<00:00,  1.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.05s/it]\n","                   all         50         50      0.853          1       0.95      0.753\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      15/19      13.8G    0.02171    0.02308    0.01032        106        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.11s/it]\n","                   all         50         50      0.922      0.814      0.946      0.736\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      16/19      13.8G    0.02127    0.02274    0.00979        139        640: 100% 37/37 [00:26<00:00,  1.39it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.10s/it]\n","                   all         50         50      0.877      0.971      0.937      0.737\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      17/19      13.8G    0.01931    0.02218   0.008987        115        640: 100% 37/37 [00:26<00:00,  1.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.42it/s]\n","                   all         50         50      0.971       0.88       0.95      0.771\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      18/19      13.8G    0.01916    0.02197   0.008557        103        640: 100% 37/37 [00:26<00:00,  1.38it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.41it/s]\n","                   all         50         50      0.935       0.98      0.995      0.785\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      19/19      13.8G    0.01929    0.02158   0.007395        140        640: 100% 37/37 [00:27<00:00,  1.36it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.44it/s]\n","                   all         50         50      0.981      0.957      0.995      0.797\n","\n","20 epochs completed in 0.163 hours.\n","Optimizer stripped from runs/train/exp/weights/last.pt, 42.2MB\n","Optimizer stripped from runs/train/exp/weights/best.pt, 42.2MB\n","\n","Validating runs/train/exp/weights/best.pt...\n","Fusing layers... \n","Model summary: 212 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.15it/s]\n","                   all         50         50      0.981      0.956      0.995      0.797\n","               compost         50         35          1      0.952      0.995      0.718\n","                 paper         50          5      0.969          1      0.995      0.864\n","               plastic         50          5          1      0.873      0.995      0.825\n","                 trash         50          5      0.956          1      0.995      0.779\n","Results saved to \u001b[1mruns/train/exp\u001b[0m\n"]}],"source":["!python train.py --img 640 --batch 32 --epochs 20 --data /content/datasets/HRI-RA-3/data.yaml --weights yolov5m.pt --cache"]},{"cell_type":"markdown","metadata":{"id":"AcIRLQOlA14A"},"source":["## Evaluation 📏"]},{"cell_type":"markdown","source":["### Test normally\n","\n","\n","> *FP16 half-precision can be used during inference by adding the **--half** flag!*"],"metadata":{"id":"PdFsjO1unKjK"}},{"cell_type":"code","source":["!python val.py --weights /content/drive/MyDrive/EAI/HRI_RA/GarbageDet/best.pt --data /content/datasets/HRI-RA-3/data.yaml --img 640 --half"],"metadata":{"id":"0PBGnBRsm8nH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689345383703,"user_tz":-120,"elapsed":16560,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"}},"outputId":"ee6dde11-f8b0-495e-bd61-f5dd9195afb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/datasets/HRI-RA-3/data.yaml, weights=['/content/drive/MyDrive/EAI/HRI_RA/GarbageDet/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n","YOLOv5 🚀 v7.0-193-g485da42 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 212 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/HRI-RA-3/valid/labels.cache... 50 images, 0 backgrounds, 0 corrupt: 100% 50/50 [00:00<?, ?it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:03<00:00,  1.61s/it]\n","                   all         50         50      0.981      0.956      0.995      0.797\n","               compost         50         35          1      0.952      0.995      0.718\n","                 paper         50          5      0.969          1      0.995      0.864\n","               plastic         50          5          1      0.873      0.995      0.825\n","                 trash         50          5      0.956          1      0.995      0.779\n","Speed: 0.2ms pre-process, 30.2ms inference, 6.6ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1mruns/val/exp\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### Post Pruning ✂"],"metadata":{"id":"-uOmVHPJiB8o"}},{"cell_type":"markdown","source":["We repeat the above test with a pruned model by using the *torch_utils.prune()* command. We create an update version of *val.py* file to prune the model."],"metadata":{"id":"Z1a4pxNQokxX"}},{"cell_type":"code","source":["!mv val.py val_old.py\n","!cp val_old.py val.py\n","!mv val.py val_prune.py\n","!mv val_old.py val.py"],"metadata":{"id":"NIg10_LYo-RL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Add this lines of code to the new created *val_prune.py* file:\n","<a href=\"https://imgur.com/SoR1aJn\"><img src=\"https://i.imgur.com/SoR1aJn.png\" title=\"source: imgur.com\" /></a>"],"metadata":{"id":"JWV1M5Krq6SW"}},{"cell_type":"code","source":["# Prune\n","from utils.torch_utils import prune\n","prune(model, 0.3)"],"metadata":{"id":"1KY-_hkeiJmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python val_prune.py --weights /content/drive/MyDrive/EAI/HRI_RA/GarbageDet/best.pt --data /content/datasets/HRI-RA-2/data.yaml --img 640 --half"],"metadata":{"id":"zNQWLWBMqodA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Garbage Detection ♻"],"metadata":{"id":"AeZJpP0fiUCo"}},{"cell_type":"code","source":[" !python detect.py --weights /content/drive/MyDrive/EAI/HRI_RA/GarbageDet/best.pt --img 640 --conf 0.05 --source /content/drive/MyDrive/EAI/HRI_RA/GarbageDet/trial_imgs/real2.jpeg"],"metadata":{"id":"CnLPIylniS-D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689346949176,"user_tz":-120,"elapsed":7918,"user":{"displayName":"leonardo lavalle","userId":"15576111649696680340"}},"outputId":"5eb82d8a-b8dc-40fe-ae32-776924fecf14"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/EAI/HRI_RA/GarbageDet/best.pt'], source=/content/drive/MyDrive/EAI/HRI_RA/GarbageDet/trial_imgs/real2.jpeg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.05, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 🚀 v7.0-193-g485da42 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 212 layers, 20865057 parameters, 0 gradients, 47.9 GFLOPs\n","image 1/1 /content/drive/MyDrive/EAI/HRI_RA/GarbageDet/trial_imgs/real2.jpeg: 640x480 (no detections), 75.7ms\n","Speed: 0.7ms pre-process, 75.7ms inference, 42.0ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1mruns/detect/exp18\u001b[0m\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb","timestamp":1677672076947}],"collapsed_sections":["yNveqeA1KXGy","PdFsjO1unKjK","-uOmVHPJiB8o"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}